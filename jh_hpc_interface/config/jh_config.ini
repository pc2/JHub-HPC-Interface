# Configuration file for JHub-HPC-Interface

[general]
work_dir = /scratch/hpc-group

jupyterhub_ip = 192.168.56.103
jupyterhub_api_url = http://127.0.0.1:${ssh_config:ssh_tunnel_dst_api_port}/hub/api

log_file = ${work_dir}/application.log
log_level = DEBUG

[maintenance]
maintenance = False
maintenance_user = mawi

[workload_manager]
# Instead of the actual job id, just enter $JOBID.
# To escape '%', use '%%'
cmd_run_job = sbatch --parsable
cmd_run_job_debug = sbatch --parsable

cmd_kill_job = scancel $JOBID

cmd_job_state = squeue -ho %%.T -j $JOBID

cmd_job_get_exec_node = squeue -ho %%.B -j $JOBID
# Maybe it's something like: node[0-9]-[0-9]
execnode_regex = slurmnode

job_state_is_running = RUNNING, COMPLETING
job_state_is_stopped = COMPLETED
job_state_is_planned = PLANNED, PLANNING

[ssh_config]
# Do you want to make the JupyterHub API available on the compute node?
ssh_tunnel_api = True
ssh_tunnel_src_api_port = 8081
# On which port should I map the JupyterHub API (127.0.0.1:????) See $jupyterhub_api_url
ssh_tunnel_dst_api_port = 8083

ssh_tunnel_user = tunnelbot
ssh_keypath = /home/mawi/.ssh/id_rsa

[singularity]
use_singularity = False
singularity_container_compute = /scratch/hpc-group/JHub-HPC-Interface/singularity/compute_jupyter.sif
singularity_container_gpu = 

#comma-seperated
singularity_extra_args = --bind /scratch/

singularity_use_overlay = True
# auto = automatically create an overlay (e.g. $singularity_overlay_location/mawi.img) in directory $singularity_overlay_location, spec = always use one specified overlay. (Should be a valid overlay of course)
singularity_overlay_c = auto
singularity_overlay_location = /scratch/hpc-group/JHub-HPC-Interface/singularity/
singularity_overlay_size = 2048

