#!/bin/bash
SDIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" > /dev/null 2>&1 && pwd)"
[[ ! -f $SDIR/jh_config ]] && >&2 echo "All wrapper scripts should be placed in the same directory!" && exit 1
source $SDIR/jh_config
# Purpose:
# Wrapper Script to start a wlm job with JupyterHub batchspawner
# The output should ONLY contain the ccs job id so that the batchspawner can extract it!!! (e.g. 5699532)
# if JupyterHub (batchspawner) calls this script, following environment variables will pass it: JUPYTERHUB_USER (Username), JUPYTERHUB_API_TOKEN, ...
# This script copies a singularity container for each new user in $home_dir directory

# Following procedure:
# 1. Writes from STDIN to $WRAP_TMP_JOB
# 2. Check whether a user is new and create an empty file if so 
# 3. calling the workload manager command to start a batch job with $WRAP_TMP_JOB
# 4. The output of the workload manager call should be the unique job id so that the JupyterHub can control the job (show job status, kill job, ...)
# 5. creating job directory with job specific information (start date/time, compute node hostname, username) and create a home directory if the user is new
# 6. And now it's time for $WRAP_TMP_JOB 

### CHECK WHETHER CLUSTER IS IN MAINTENANCE MODE
if [[ $is_cluster_in_maintenance == true ]]; then
	if [[ ! $JUPYTERHUB_USER == $maintenance_only_user ]]; then
		>&2 echo "The HPC cluster is currently in maintenance mode!"
		exit 1	
	else
		create_log_entry "WARNING" "Maintenance user $JUPYTERHUB_USER ($maintenance_only_user) started a job..."
	fi
fi

#### TEMPLATE BATCH SCRIPT
input_=$(< /dev/stdin)
if $creating_user_homes; then
	WRAP_TMP_JOB=$(mktemp --tmpdir=$home_dir/ --suffix=.$JUPYTERHUB_USER)
else
	WRAP_TMP_JOB=$(mktemp --tmpdir=$scratch_dir/ --suffix=.$JUPYTERHUB_USER)
fi
### READING FROM STDIN AND WRITE IT TO $WRAP_TMP_JOB
echo "$input_" > $WRAP_TMP_JOB

# This overlay will be used as a persitent overlay for the user. All changes affected by the user will be stored in the overlay 
# The default overlay size can be changed in the configuration file jh_config and will be applied when a new user register
function create_empty_singularity_overlay () {

    create_log_entry "DEBUG" "[Singularity Overlay] Trying to create singularity overlay with ext3 filesystem for user $JUPYTERHUB_USER"
    create_overlay_output=$(eval $create_overlay_cmd)
    if [[ ! $? == 0 ]]; then
        create_log_entry "ERROR" "Error creating an overlay. Output: $create_overlay_output"
        >&2 echo "Error creating an overlay. Output: $create_overlay_output"
        exit 1
    fi
    create_ext3_overlay_output=$(eval $create_ext3_overlay_cmd)
    if [[ ! $? == 0 ]]; then
        create_log_entry "ERROR" "Error while creating an ext3 filesystem. Output: $create_ext3_overlay_output"
        >&2 echo "Error while creating an ext3 filesystem. Output: $create_ext3_overlay_output"
        exit 1
    fi
	create_log_entry "INFO" "[Singularity Overlay] Empty singularity overlay with ext3 filesystem for user $JUPYTERHUB_USER successfully created"
}

function initialize_user_env () {

	if $creating_user_homes; then
        [[ ! -d $home_dir ]] && mkdir -p $home_dir
		if [[ ! -d $home_dir/$JUPYTERHUB_USER ]]; then
			create_log_entry "INFO" "[Start] User $JUPYTERHUB_USER seems to be new.. Creating home directory $user_home_dir"
			# create user and log directory
			mkdir -p $user_log_directory
			# create singularity overlay for user so that user changes are permanent
			if $use_singularity; then
				create_empty_singularity_overlay $home_dir/$JUPYTERHUB_USER/overlay.img
			fi
		else
            # User is already known
			[[ ! -d $user_log_directory ]] && mkdir $user_log_directory
			[[ -f $user_log_directory/lastlog ]] && rm $user_log_directory/lastlog
			create_log_entry "INFO" "[Start] User $JUPYTERHUB_USER is already known... Using existing notebook directory and singularity overlay"
		fi
	else
		create_log_entry "DEBUG" "Not creating a user home directory for user $JUPYTERHUB_USER"
	fi
}

### CHECK PREVIOUSLY STARTED JOB. SOMETIMES THE WORKLOAD MANAGER IS BR0KEN 
function check_started_job () {

	# Checking job state after 15 seconds
	create_log_entry "DEBUG" "Now checking job state!"
    job_state=$(eval $cmd_job_state)
	# if the job state is something specified in cluster_job_is_planned
	if [[ $cluster_job_is_planned == *"$job_state"* ]]; then
		create_log_entry "ERROR" "[Start] Job state for job $JOBID changed to planned! Kill the job.."
		>&2 echo "It seems your requested resources are currently not available. Please start a notebook server with less resources or try again later."
        eval $cmd_kill_job
		exit 1
	elif [[ $cluster_job_is_stopped == *"$job_state"* ]]; then
		create_log_entry "ERROR" "[Start] The notebook job $JOBID is lost after 15 seconds"
		>&2 echo "Hmm... Something went wrong. Please ask the system administrator!"
        eval $cmd_kill_job
		exit 1
	else
		create_log_entry "DEBUG" "Job status for user $JUPYTERHUB_USER with ID $JOBID checked. Everything seems fine until here"
	fi
}

# Start job 
function run_wlm_job () {

    # mark the job file as executable
	chmod 700 $WRAP_TMP_JOB

	if $creating_user_homes; then
		[[ ! -d $user_log_directory ]] && mkdir $user_log_directory
	fi

    # Run the Workload Manager job
	if $enable_debug_mode; then
        RUN_JOB_OUTPUT=$(eval $cmd_run_job_debug $WRAP_TMP_JOB)
	else
        RUN_JOB_OUTPUT=$(eval $cmd_run_job $WRAP_TMP_JOB)
	fi

	create_log_entry "DEBUG" "[Workload Manager] Output: $RUN_JOB_OUTPUT"

	# write job id to output -> Used by the JupyterHub to control the job
	# After output, the JupyterHub Server extracts the Job ID with the method: parse_job_id . See class CustomHPCSpawner
	JOBID=$(echo $RUN_JOB_OUTPUT | grep -Po "\\d+")

    if [[ ! $? -eq 0 ]]; then
        create_log_entry "ERROR" "[Start] Cannot filter a number from job allocation output."
    fi

	# check whether output is zero 0
	if [[ "$JOBID" == "0" ]]; then
		create_log_entry "ERROR" "[Start] JOBID_OUTPUT is zero??? Kill the job."
		>&2 echo "Cannot extract Job ID. Please try again."
        exit 1
	fi

    # So the only output of this wrapper script (called by the JupyterHub) is the job id.
	echo $JOBID
	create_log_entry "INFO" "[Start] Running notebook job for user $JUPYTERHUB_USER with ID $JOBID"

	create_log_entry "DEBUG" "Checking job $JOBID in 15 seconds..."
	sleep 15
	check_started_job
}

### START FUNCTIONS
initialize_user_env
run_wlm_job
